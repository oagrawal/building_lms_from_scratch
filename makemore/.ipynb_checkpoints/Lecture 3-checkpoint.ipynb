{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f4799d-8ea7-4510-8032-d96d9d2f51b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narray based approach is not scalable; matrix grows exponentially with context length\\ngoing from bigram to MLP to predict next character (following Bengio et al: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\\n-> reserach paper made word level model, but well stick to characters\\n-> for every word in vocab (17,000), a 30-dim feature vector\\n-> very crowded\\n-> these embeddings are init randomly, but are tuned using backprop\\n-> max log liklihood of training data\\n-> \"out of distributiuon\": youre in inference time and you see an exmaple that you haven\\'t encountered in training time\\n-> embeddings approach, lets you get around \"out of distribution\" problem\\n----> A and the are interchangeable, embeddings are nearby for a and the, transfer knowledge through embeddings\\n-> hyperparameter-> design choice for the neural network, up to neural network designer\\n-> \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "array based approach is not scalable; matrix grows exponentially with context length\n",
    "going from bigram to MLP to predict next character (following Bengio et al: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "-> reserach paper made word level model, but well stick to characters\n",
    "-> for every word in vocab (17,000), a 30-dim feature vector\n",
    "-> very crowded\n",
    "-> these embeddings are init randomly, but are tuned using backprop\n",
    "-> max log liklihood of training data\n",
    "-> \"out of distributiuon\": youre in inference time and you see an exmaple that you haven't encountered in training time\n",
    "-> embeddings approach, lets you get around \"out of distribution\" problem\n",
    "----> A and the are interchangeable, embeddings are nearby for a and the, transfer knowledge through embeddings\n",
    "-> hyperparameter-> design choice for the neural network, up to neural network designer\n",
    "-> \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3a8f74-a059-46d5-af98-18a0a6ed8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4889f0-9b4a-4593-aab4-75cea301eb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ddb937-949a-4792-9206-b9bc89f1107e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d53ff3-6a38-46bb-bbda-97945e1174f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89503e82-7208-429e-aac0-afdd32e7177f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "  \n",
    "  print(w)\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "    context = context[1:] + [ix] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1fd7675-9658-450e-a659-8fe1bc75231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e3ffbf-3802-45df-9ad2-459448fbd277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embeddings space\n",
    "# paper has 17K words, we only have 27 characters\n",
    "# lets make embedding space 2 dimensional\n",
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b55ba79-f48e-46d7-8158-9b42d5650265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2817, -0.0477])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embed one word\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89e4012a-babf-421e-8d7b-19fef218725a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor of type LongTensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m F\u001b[38;5;241m.\u001b[39mone_hot(torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;241m5\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m27\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor of type LongTensor."
     ]
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
